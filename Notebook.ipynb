{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra For Machine Learning\n",
    "----------------\n",
    "\n",
    "## Introduction\n",
    "------------------\n",
    "\n",
    "Numerical linear algebra (and numerical analysis more generally) was one of thoses courses that I learned, thought was boring and hated. Only with maturity that comes with ages was I able to understand and appreciate the true power of numerical linear alebra.  Infact *understanding (distribued) linear algebra is probably one of the most important and useful tools I have ever learned.*  It has allowed me to contribute to open source libraries for scientific computing and understand how big data and machine learning systems. In scientific computing and machine learning one is interested in **how to approximate a function** $f(x)$.  Numerical analysis and statistics concerns itself with **how good is our approximation to** $f(x)$?\n",
    "\n",
    "One learns in Calculus to approximate smooth functions \\$f(x)\\$ about some point \\$x_0\\$ using a special polynomials called, Taylor series,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\, = \\sum_{n=0}^{\\infty} \\, a_n (x - x_0)^{n}\n",
    "\\end{equation}\n",
    "\n",
    "The approximation,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\, \\simeq \\, f_N(x) \\, = \\, a_0  \\, + \\, a_1 (x - x_0) \\, + \\,a_2 (x - x_0)^2 \\, + \\ldots \\, + \\, a_N (x - x_0)^N\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We learn we can find the coefficients to the series by the equation,\n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "a_{n} \\; = \\; \\frac{(-1)^{n} }{n!} \\, f^{n}(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "One learns there is a radius of convergence $R$ such that within $(x_0 - R, x_0 + R)$ (one should test the end points too) the series converges to $f(x)$ and outside that it does not.  Taylor series were originaly developed to <a href=\"https://en.wikipedia.org/wiki/Power_series_solution_of_differential_equations\">approximate solutions to differential equations</a>. As science and engineering progressed the differential equations become more complicated and harder so solve.  Other approximations methods were invented like <a href=\"https://en.wikipedia.org/wiki/Fourier_series\">Fourier Series</a>:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\, \\simeq \\, f_N(x) \\, = \\, \\frac{a_0}{2} \\, + \\sum_{k=1}^{N} \\left(a_n \\cos \\left(\\frac{2 \\pi k x}{L} \\right) + b_n \\sin \\left(\\frac{2 \\pi k x}{L} \\right) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The functions $\\cos(\\frac{2 \\pi k x}{L})$ and $\\sin(\\frac{2 \\pi k x}{L})$ form an **orthogonal basis for the Hilbert Space** $L^{2}([-L,L])$.  **Coefficients for the approximations are determined projecting the function onto the basis functions:**\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "a_n \\; = \\; \\frac{2}{L} \\, \\int_{-L}^{L} \\, f(x) \\,\\cos \\left(\\frac{2 \\pi n x}{L} \\right) \\, dx \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "b_n \\; = \\; \\frac{2}{L} \\, \\int_{-L}^{L} \\, f(x) \\,\\sin \\left(\\frac{2 \\pi n x}{L} \\right) \\, dx \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We show how the second equation comes about saying,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{-L}^{L} \\sin \\left(\\frac{2 \\pi n x}{L} \\right) \\, \\left[\\frac{a_0}{2} \\, + \\sum_{k=1}^{N} \\left(a_n \\cos \\left(\\frac{2 \\pi k x}{L} \\right) + b_n \\sin\\left(\\frac{2 \\pi k  x}{L} \\right) \\right) \\right] \\, dx \\; &= \\; \\int_{-L}^{L} \\sin \\left(\\frac{2 \\pi n x}{L} \\right) \\, f(x) \\, dx \\\\\n",
    "b_n \\, \\int_{-L}^{L} \\sin \\left(\\frac{2 \\pi n x}{L} \\right)^2 \\, dx  \\; &= \\; \\int_{-L}^{L} \\sin \\left(\\frac{2 \\pi n x}{L} \\right) \\, f(x) \\, dx \\\\\n",
    "b_n \\, \\frac{L}{2} \\; &= \\;  \\int_{-L}^{L} \\, f(x) \\,\\sin \\left(\\frac{2 \\pi n x}{L} \\right) \\, dx \n",
    "\\end{align}\n",
    "\n",
    "With the advent of the computer and seemingly unlimited computational resources numerical methods like finite difference and finite element methods were invented to approximate solutions to differential equations. The finite element method is one that is particularly dear to my heart and has concepts that have proved useful in understanding models in statistics and machine learning, particularly, <a href=\"https://en.wikipedia.org/wiki/Generalized_additive_model\">Generalized Addative Models</a>.  One of the central requirements of both approximation methods is:\n",
    "\n",
    "1. **Projecting the function onto a finite dimensional basis**\n",
    "\n",
    "2. **Solving for the co-efficients in representation of the function in this basis**\n",
    "\n",
    "The last step often requires **solving a system of linear equations:** \n",
    "\n",
    "\\begin{equation}\n",
    "S x \\, = \\, b\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This solving of linear systems occurs because the basis is not orthogonal.  If the basis is orthogonal, then the matrix is **diagonal** and the matrix can be inverted by hand, leading to equations like the Fourier coefficient equations.\n",
    "\n",
    "\n",
    "In the next section well go into the basics of these two concepts for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "----------------------------\n",
    "\n",
    "\\noindent\n",
    "The target variable $y$ has continuous values, but here the feature is a vector \n",
    "$\\textbf{x}_{i} = [1, x_{1}, x_{2}, \\ldots, x_{p}] \\; \\in \\, \\mathbb{R}^{p+1}$, that can have continuous and categorical variables.   Then the target is described by,\n",
    "\n",
    "\\begin{equation}\n",
    "y_{i} \\; =\\; \\boldsymbol  \\theta^{T} \\textbf{x}_{i} + \\epsilon_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\noindent\n",
    "Where $\\epsilon_{i} \\, \\sim \\, N(0,1)$ are independent and normal distributed with mean 0 and variance 1.  We have our **training set** $D_{\\text{TR}} \\, = \\,  \\left\\{ (\\textbf{x}_{i}, \\, y_{i})  \\right\\}_{i=1}^{n}$.  Or  $D_{\\text{TR}} \\, = \\, (\\textbf{X}, \\textbf{y}$), where $\\textbf{X} \\in \\mathbb{R}^{n \\times (p+1)}$ and $\\textbf{y} \\in \\mathbb{R}^{n}$. Our goal is to approximated $y_i$ with $h_{\\boldsymbol \\theta}(\\textbf{x}_{i})$ where the model is of the form,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\boldsymbol \\theta}(\\textbf{x}_{i}) \\, = \\, \\boldsymbol \\theta^{T} \\textbf{x}_{i} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We find $\\boldsymbol \\theta$ by minimizing the cost function $J \\left( \\boldsymbol \\theta \\right)$,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\boldsymbol \\theta}\n",
    " \\; &= \\; \n",
    " \\min_{\\boldsymbol \\theta} \\, \\frac{1}{2} \\sum_{i=1}^{n} \\left( h_{\\boldsymbol \\theta}(\\textbf{x}_{i})  - y_{i} \\right)^{2} \\\\\n",
    "&= \\; \n",
    " \\min_{\\boldsymbol \\theta} \\, \\frac{1}{2} \\Vert  h_{\\boldsymbol \\theta}(\\textbf{x})- \\textbf{y} \\Vert^{2} \\\\\n",
    " &= \\; \n",
    " \\min_{\\boldsymbol \\theta} \\, J \\left( \\boldsymbol \\theta \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Mininmizing $J \\left( \\boldsymbol \\theta \\right)$ is equivalent to setting the $\\nabla J \\left( \\boldsymbol \\theta \\right) \\; = \\; 0$.  We can expand the cost function,\n",
    "\n",
    "\\begin{align}\n",
    "J(\\boldsymbol \\theta ) \n",
    "\\; &= \\; \n",
    "\\frac{1}{2} \\, \\Vert \\textbf{X} \\boldsymbol \\theta - \\textbf{y} \\Vert^{2} \\\\\n",
    "&= \\; \n",
    " \\min_{\\boldsymbol \\theta} \\, \\frac{1}{2} \\Vert \\textbf{X} \\boldsymbol \\theta - \\textbf{y} \\Vert^{2} \\\\\n",
    "&= \\; \\frac{1}{2} \\, \\left( \\textbf{X} \\boldsymbol \\theta - \\textbf{y} \\right)^{T}  \\left(\\textbf{X} \\boldsymbol \\theta - \\textbf{y} \\right) \\\\\n",
    "&= \\; \\frac{1}{2} \\, \\left( \\boldsymbol \\theta^{T} \\textbf{X}^{T} \\textbf{X}  \\boldsymbol \\theta - \\textbf{y}^{T} \\textbf{X} \\boldsymbol \\theta  - \\boldsymbol \\theta^{T} \\textbf{X}^{T} \\textbf{y}  + \\textbf{y}^{T} \\textbf{y}  \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Taking the gradient of both sides we then have,\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol \\nabla J(\\boldsymbol \\theta ) \n",
    "\\; = \\;\n",
    "\\textbf{X}^{T} \\left( \\textbf{X} \\boldsymbol \\theta - \\textbf{y} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Setting the above equal to zero yields the linear system of equations,\n",
    "\n",
    "\n",
    "\\begin{align} \\label{eq:NormalEquations}\n",
    "\\left(\\textbf{X}^{T}\\textbf{X}\\right) \\,\n",
    "\\hat{\\boldsymbol \\theta}\n",
    "\\; = \\;\n",
    "\\textbf{X}^{T} \\textbf{y} \\\\\n",
    "S \\,\n",
    "\\hat{\\boldsymbol \\theta}\n",
    "\\; = \\;\n",
    "b\\\\\n",
    "\\end{align}\n",
    "\n",
    "When the features have been scaled (as we have assumed), the above matrix is the **covariance matrix** ($S \\, = \\, \\textbf{X}^{T}\\textbf{X} \\, \\in \\mathbb{R}^{(p+1) \\times (p+1)}$) \n",
    "\n",
    "\\begin{equation}\n",
    "\\left(\\textbf{X}^{T}\\textbf{X}\\right)_{i,j} \\; = \\; \\text{Cov}(x_{i},x_{j} )\n",
    "\\end{equation}\n",
    "\n",
    "The covariance matrix is symmetric, but if the two features are highly correlated then two rows in the matrix will be nearly identical and the matrix will be nearly singular.  The vector $b \\, = \\, \\textbf{X}^{T} \\textbf{y} $ is like a projection on to each of the features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html)\n",
    "    \n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.cholesky.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Scikit-Learn](https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/linear_model/base.py#L367) implementation utilizes Scipy's [least square solver](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) if the matrix is dense calls [BLAS/LAPACK](https://github.com/numpy/numpy/blob/master/numpy/linalg/umath_linalg.c.src#L3167).  If the matrix $X$ is sparse then it calls SciPy's [sparse least squares solver](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html) which uses least squres with QR factorization [LSQR](https://github.com/scipy/scipy/blob/v1.3.2/scipy/sparse/linalg/isolve/lsqr.py#L98-L570)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "----------------------------\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tikhonov_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Systems: Alternating Least Squares\n",
    "------------------------------------\n",
    "\n",
    "http://www.mattmoocar.me/recsys/\n",
    "\n",
    "https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression With Principle Component Analysis\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
