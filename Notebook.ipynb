{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra For Machine Learning\n",
    "----------------\n",
    "\n",
    "## Introduction\n",
    "------------------\n",
    "\n",
    "Numerical linear algebra (and numerical analysis more generally) was one of thoses courses that I learned, thought was boring and hated. Only with maturity that comes with ages was I able to understand and appreciate the true power of numerical linear alebra.  Infact *understanding (distribued) linear algebra is probably one of the most important and useful tools I have ever learned.*  It has allowed me to contribute to open source libraries for scientific computing and understand how big data and machine learning systems. In scientific computing and machine learning one is interested in **how to approximate a function** \\$f(x)\\$.  Numerical analysis and statistics concerns itself with **how good is our approximation to** \\$f(x)\\$?\n",
    "\n",
    "One learns in Calculus to approximate smooth functions \\$f(x)\\$ about some point \\$x_0\\$ using a special polynomials called, Taylor series,\n",
    "\n",
    "\\$\n",
    "\\begin{equation}\n",
    "f(x) \\, = \\sum_{n=0}^{\\infty} \\, a_n (x - x_0)^{n}\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "The approximation,\n",
    "\n",
    "\\$\n",
    "\\begin{equation}\n",
    "f(x) \\, \\simeq \\, f_N(x) \\, = \\, a_0  \\, + \\, a_1 (x - x_0) \\, + \\,a_2 (x - x_0)^2 \\, + \\ldots \\, + \\, a_N (x - x_0)^N\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "We learn we can find the coefficients to the series by the equation,\n",
    "\n",
    "\\$\n",
    "\\begin{equation} \n",
    "a_{n} \\; = \\; \\frac{(-1)^{n} }{n!} \\, f^{n}(x_0)\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "One learns there is a radius of convergence \\$R\\$ such that within \\$(x_0 - R, x_0 + R)\\$ (one should test the end points too) the series converges to $f(x)$ and outside that it does not.  Taylor series were originaly developed to <a href=\"https://en.wikipedia.org/wiki/Power_series_solution_of_differential_equations\">approximate solutions to differential equations</a>. As science and engineering progressed the differential equations become more complicated and harder so solve.  Other approximations methods were invented like <a href=\"https://en.wikipedia.org/wiki/Fourier_series\">Fourier Series</a>:\n",
    "\n",
    "\\$\n",
    "\\begin{equation}\n",
    "f(x) \\, \\simeq \\, f_N(x) \\, = \\, \\frac{a_0}{2} \\, + \\sum_{n=1}^{N} \\left(a_n \\cos(\\frac{2 \\pi x}{L}) + b_n \\sin(\\frac{2 \\pi x}{L}) \\right)\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "The functions \\$\\cos(\\frac{2 \\pi x}{L})\\$ and \\$\\sin(\\frac{2 \\pi x}{L})\\$ form an **orthogonal basis for the Hilbert Space** \\$L^{2}([-L,L])\\$.  **Coefficients for the approximations are determined projecting the function onto the basis functions:**\n",
    "\n",
    "\\$\n",
    "\\begin{equation}\n",
    "a_n \\; = \\; \\frac{2}{L} \\, \\int_{-L}^{L} \\, f(x) \\,\\cos(\\frac{2 \\pi x}{L}) \\, dx \n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "\\begin{equation}\n",
    "b_n \\; = \\; \\frac{2}{L} \\, \\int_{-L}^{L} \\, f(x) \\,\\sin(\\frac{2 \\pi x}{L}) \\, dx \n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "\n",
    "We show how the second equation comes about saying,\n",
    "\n",
    "\\$\n",
    "\\begin{equation}\n",
    "\\int_{-L}^{L} \\sin(\\frac{2 \\pi x}{L}) \\, \\left[\\frac{a_0}{2} \\, + \\sum_{n=1}^{N} \\left(a_n \\cos(\\frac{2 \\pi x}{L}) + b_n \\sin(\\frac{2 \\pi x}{L}) \\right) \\right] \\, dx \\; = \\; \\int_{-L}^{L} \\sin(\\frac{2 \\pi x}{L}) \\, f(x) \\, dx \\\\\n",
    "b_n \\, \\int_{-L}^{L} \\sin(\\frac{2 \\pi x}{L})^2 \\, dx  \\; = \\; \\int_{-L}^{L} \\sin(\\frac{2 \\pi x}{L}) \\, f(x) \\, dx \\\\\n",
    "b_n \\, \\frac{L}{2} \\; = \\;  \\int_{-L}^{L} \\, f(x) \\,\\sin(\\frac{2 \\pi x}{L}) \\, dx \n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "With the advent of the computer and seemingly unlimited computational resources numerical methods like finite difference and finite element methods were invented to approximate solutions to differential equations. The finite element method is one that is particularly dear to my heart and has concepts that have proved useful in understanding models in statistics and machine learning, particularly, <a href=\"https://en.wikipedia.org/wiki/Generalized_additive_model\">Generalized Addative Models</a>.  One of the central requirements of both approximation methods is:\n",
    "\n",
    "1. **Projecting the function onto a finite dimensional basis**\n",
    "\n",
    "2. **Solving for the co-efficients in representation of the function in this basis**\n",
    "\n",
    "The last step often requires **solving a system of linear equations:** \n",
    "\\$\n",
    "\\begin{equation}\n",
    "A x \\, = \\, b\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "This solving of linear systems occurs because the basis is not orthogonal.  If the basis is orthogonal, then the matrix is **diagonal** and the matrix can be inverted by hand, leading to equations like the Fourier coefficient equations.\n",
    "\n",
    "\n",
    "In the next section well go into the basics of these two concepts for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "----------------------------\n",
    "\n",
    "\\$\n",
    "\\begin{equation} \n",
    "\\widehat{y}(x)\\; = \\; \\theta_0 + \\theta_1 x + \\epsilon\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "\n",
    "\\$\n",
    "\\begin{equation} \n",
    "\\widehat{y}(x) \\; = \\; \\boldsymbol \\theta^T \\textbf{x} + \\epsilon\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "\n",
    "\\$\n",
    "\\begin{equation} \n",
    "\\boldsymbol \\theta^T \\textbf{x}_i \\; = \\; y_i\n",
    "\\end{equation}\n",
    "\\$\n",
    "\n",
    "\n",
    "\\$\n",
    "\\begin{equation} \n",
    "X \\boldsymbol \\theta  \\; = \\; \\textbf{y} \n",
    "\\end{equation}\n",
    "\\$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "----------------------------\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tikhonov_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Systems: Alternating Least Squares\n",
    "------------------------------------\n",
    "\n",
    "http://www.mattmoocar.me/recsys/\n",
    "\n",
    "https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression With Principle Component Analysis\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
